# AI-Based Project Ideas: Image-to-Text & Image-to-Image
This repository is for providing some ideas to develop.

<details>
  <summary>1ï¸âƒ£ Image Captioning (General)</summary>
  
  ### ğŸ§  Foundational Models
  - **GIT (Generative Image-to-Text Transformer)** â€“ Uses a vision encoder and text decoder for image captioning.
  - **BLIP (Bootstrapped Language-Image Pretraining)** â€“ Enhances vision-language pretraining for better text generation.
  - **CoCa (Contrastive Captioners)** â€“ Combines contrastive learning with an encoder-decoder framework.

  ### ğŸ† Benchmark Datasets
  - **MS COCO Captioning Dataset** (~330K images, five captions per image) â€“ [Link](https://cocodataset.org/#home)
  - **Flickr30K** (~30K images, five captions per image) â€“ [Link](https://shannon.cs.illinois.edu/DenotationGraph/)

  ### ğŸ“– Reading Materials
  - [GIT Paper (arXiv)](https://arxiv.org/abs/2205.14100)
  - [CoCa Paper (arXiv)](https://arxiv.org/abs/2205.01917)
</details>

<details>
  <summary>2ï¸âƒ£ Medical Image Captioning (CT Scan Report Generation)</summary>
  
  ### ğŸ§  Foundational Models
  - **CT2Rep** â€“ Automated **radiology report generation** for 3D medical imaging.
  - **Multi-modal Transformers (e.g., MedViLL, BioViL-T)** â€“ Optimized for **vision-language tasks** in the medical domain.

  ### ğŸ† Benchmark Datasets
  - **MIMIC-CXR** (Chest X-ray dataset with reports) â€“ [Link](https://physionet.org/content/mimic-cxr/2.0.0/)
  - **IU X-ray Dataset** (Chest X-rays with structured radiology reports) â€“ [Link](https://openi.nlm.nih.gov/)

  ### ğŸ“– Reading Materials
  - [CT2Rep: Automated Radiology Report Generation](https://arxiv.org/html/2403.06801v1)
  - [Multi-modal Transformer for Medical Image Captioning](https://www.nature.com/articles/s41598-024-69981-5)
</details>

<details>
  <summary>3ï¸âƒ£ AI-Based Trending Hashtag Generator for Instagram</summary>
  
  ### ğŸ§  Foundational Models
  - **CLIP (Contrastive Language-Image Pertaining)** â€“ Matches images with text descriptions (great for hashtag relevance).
  - **BLIP (Bootstrapped Language-Image Pertaining)** â€“ Can generate **context-aware captions and hashtags**.

  ### ğŸ† Benchmark Datasets
  - **Instagram Hashtag Dataset** â€“ [Example Dataset on Kaggle](https://www.kaggle.com/datasets/nikhilb25/instagram-data)
  - **Hateful Memes (Facebook Research)** â€“ Useful for **image-text** associations â€“ [Link](https://www.drivendata.org/competitions/64/hateful-memes/)

  ### ğŸ“– Reading Materials
  - [Using CLIP for Hashtag Prediction](https://arxiv.org/pdf/2103.00020.pdf)
  - [AI-Powered Hashtag Optimization](https://www.semanticscholar.org/paper/Hashtag-Prediction-for-Social-Media-Posts-Using-Liu-Hua/9735e1f25dbb8d7ec1c7b7714e1f256d038d46d6)
</details>

<details>
  <summary>4ï¸âƒ£ Video/Text-to-Animation Generation (Creative ğŸ¥âœ¨) - Milad is going to update this section </summary>
  
  ### ğŸ§  Foundational Models
  
  ### ğŸ† Benchmark Datasets
  
  ### ğŸ“– Reading Materials
</details>




